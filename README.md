# Runpod Serverless TTS Worker (Advanced)

This project provides a serverless worker for Runpod.io to perform text-to-speech using Coqui's XTTS-v2 model. This version includes advanced audio pre-processing and post-processing for higher quality output.

## Files

- `Dockerfile`: Defines the container image for the worker, including `ffmpeg`.
- `handler.py`: The core worker script. It loads the TTS model and defines the `handler` function that processes incoming jobs, including downloading reference audio, pre/post-processing, and uploading the final result.
- `requirements.txt`: Lists all the Python packages required for the worker to run.

## How it Works

1.  **Model Loading**: The `handler.py` script loads the `XTTS-v2` model into memory when the worker starts. The model is automatically downloaded on the first run.
2.  **Job Processing**: The `handler` function receives a `job` object from Runpod. It expects the job `input` to be a JSON object containing:
    - `text` (required): The text to be converted to speech.
    - `reference_audio_url` (required): A public URL to a `.wav` file to be used for voice cloning.
    - `language` (optional): The language of the text (e.g., `en`, `pt`). Defaults to `pt`.
    - `output_format` (optional): `wav` or `mp3`. Defaults to `wav`.
    - Other TTS parameters (optional): `temperature`, `speed`, `top_k`, etc.
3.  **Audio Processing Pipeline**:
    - The reference audio is downloaded from the provided URL.
    - **Pre-processing**: The reference audio is converted to 16kHz mono, ideal for XTTS-v2.
    - **TTS Generation**: The raw audio is generated by the model.
    - **Post-processing**: The generated audio (24kHz) is upsampled to 44.1kHz stereo and normalized to produce a cleaner, richer sound.
4.  **Output**: The final, processed audio is uploaded to a secure bucket. The public URL of this audio file is returned in the job output.

## Deployment

1.  **Build the Docker Image**:
    ```bash
    docker build -t your-dockerhub-username/runpod-tts-worker-advanced:latest .
    ```

2.  **Push the Image to a Registry**:
    ```bash
    docker push your-dockerhub-username/runpod-tts-worker-advanced:latest
    ```

3.  **Create a Serverless Endpoint on Runpod**:
    - Go to your Runpod dashboard and create a new Serverless Endpoint.
    - Point it to the Docker image you just pushed.
    - Set the container disk size to at least **20 GB** to accommodate the model and dependencies.

## Usage

Once the endpoint is running, you can send jobs to it using the Runpod API.

**Example API Call (using cURL):**

```bash
curl -X POST \
  https://api.runpod.ai/v2/your-endpoint-id/runsync \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_RUNPOD_API_KEY" \
  -d '{
    "input": {
      "text": "Olá, isto é um teste do meu novo serviço de clonagem de voz.",
      "reference_audio_url": "https://your-public-url.com/path/to/reference_audio.wav",
      "language": "pt",
      "output_format": "mp3",
      "speed": 1.1
    }
  }'
```

Using the `/runsync` endpoint will wait for the job to complete and return the result directly:

```json
{
  "id": "job-id-12345",
  "status": "COMPLETED",
  "output": {
    "audio_url": "https://runpod-prod-files.s3.amazonaws.com/your-endpoint-id/job-id-12345/output_0.mp3",
    "format": "mp3"
  }
}
```
